# 1. 核心实验

| 模型                                               | B榜线上得分 |
| :------------------------------------------------- | ----------- |
| RoBERTa                                            | 0.6126      |
| RoBERTa+label smoothing                            | 0.6144      |
| RoBERTa+label smoothing+对抗扰动                   | 0.6177      |
| RoBERTa+label smoothing+对抗扰动+知识蒸馏          | 0.6191      |
| **Ensemble**                                       |             |
| [RoBERTa+NEZHA]+label smoothing+对抗扰动+知识蒸馏  | 0.6258      |
| [WoBERT+WoNEZHA]+label smoothing+对抗扰动+知识蒸馏 | 0.6278      |

RoBERTa:  [chinese_roberta_wwm_ext_L-12_H-768_A-12](https://github.com/ymcui/Chinese-BERT-wwm) (base) 

NEZHA: [NEZHA-base-WWM](https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/NEZHA-TensorFlow) (base)

WoBERT: [chinese_wobert_L-12_H-768_A-12](https://github.com/ZhuiyiTechnology/WoBERT) (base)

WoNEZHA: [chinese_wonezha_L-12_H-768_A-12](https://github.com/ZhuiyiTechnology/WoBERT) (base)

# 2. 最佳参数设置

1. SEED （随机种子）：

   + RoBERTa 和 WoBERT 为 42

   + NEZHA 和 WoNEZHA 为 43

2. 文本长度设置（主要基于文本长度的分布）：

   + text 的最大长度 (max_t_len) 为 384
   + answer 的最大长度 (max_a_len) 为 96
   + question 的最大长度 (max_q_len) 为 32

3. 训练参数（主要基于大量的调参实验）：

   + batch_size : 4

   + 梯度累积步数 (gradient_accumulation_steps) : 8

   + 迭代次数(EPOCHS) : 5  

     大部分实验在第 5 次迭代训练结束后，模型性能达到最优

   + 标签平滑的平滑因子 (label_weight) : 0.1

   + 对抗训练的 $\epsilon$  (ADV_epsilon) : WoBERT 为 0.3， WoNEZHA 为 0.1

   + Teache model 在 Student loss 中所占的权重 (teacher_rate) : 0.5

   + 温度系数 (temperature) : 10

4. 优化器设置：

   + 使用 Adam 优化器
   + 初始学习率为 3e-5
   + 使用学习率线性衰减函数，让 学习率 从第 1 个 step 到 最后一个 step ，线性衰减到 初始学习率 的 50% 。

5. beam search 参数：
   + top-k: 5